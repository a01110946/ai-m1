import streamlit as st

from langchain.agents import OpenAIFunctionsAgent, AgentExecutor
from langchain.callbacks import StreamlitCallbackHandler 
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain, LLMChain, create_qa_with_sources_chain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader, GCSDirectoryLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.prompts import MessagesPlaceholder, PromptTemplate
from langchain.schema import Document, SystemMessage, AIMessage, HumanMessage
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.tools import tool, Tool
from langchain.vectorstores.faiss import FAISS
from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory

from typing import Any, Iterator, List
import requests
from PIL import Image
from io import BytesIO

# Streamlit page config
st.set_page_config(page_title="MoradaUno Chatbot", page_icon="ü§ñ")

# Load the custom CSS
with open("llm-qa/styles.css", "r") as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

response_img = requests.get("https://github.com/fernando-m1/ai-m1/raw/main/llm-qa/M1-icon-whiteborder.png")
img = Image.open(BytesIO(response_img.content))

# Define functions
def text_splitter():
  return RecursiveCharacterTextSplitter(
    separators=["#","##", "###", "\n\n", "####","\n","."],
    chunk_size=1500,
    chunk_overlap=500,
  )

def gcs_loader(bucket, project_name, text_splitter, prefix=None):
    loader = GCSDirectoryLoader(bucket=bucket, project_name=project_name, prefix=prefix, loader_func=UnstructuredMarkdownLoader)
    docs = loader.load_and_split(text_splitter)
    return docs
  
def create_retriever(docs, top_k_results):
  embeddings = OpenAIEmbeddings()
  vectorstore = FAISS.from_documents(docs, embeddings)
  retriever = vectorstore.as_retriever(search_kwargs={"k": top_k_results})
  return retriever
  
# Load documents
text_splitter = text_splitter()
gcs_project_name = "legal-ai-m1"
gcs_bucket = "moradauno-corpus-demo"

m1_docs = gcs_loader(gcs_bucket, gcs_project_name, text_splitter, prefix='M1_General/')
productos_docs = gcs_loader(gcs_bucket, gcs_project_name, text_splitter, prefix='Productos/')
legal_docs = gcs_loader(gcs_bucket, gcs_project_name, text_splitter, prefix='Legal/')
m1app_docs = gcs_loader(gcs_bucket, gcs_project_name, text_splitter, prefix='M1App/')

# Create retrievers
llm = ChatOpenAI(temperature=0, streaming=True, model_name="gpt-4")
embedding = OpenAIEmbeddings()

m1_retriever = create_retriever(m1_docs, 3) 
productos_retriever = create_retriever(productos_docs, 6)
legal_retriever = create_retriever(legal_docs, 10)
m1app_retriever = create_retriever(m1app_docs, 5)

# Define chains
chain_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

condense_question_template = """Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\
    Make sure to avoid using any unclear pronouns.
    
    Chat History:
    {chat_history}
    Follow Up Input: {question}
    Standalone question:"""

CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_question_template)

condense_question_chain = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)

qa_chain = create_qa_with_sources_chain(llm)

doc_prompt = PromptTemplate(
  template="""<<SYS>> \n T√∫ nombre es Leyla. Eres un chatbot cuya tarea es responder preguntas sobre Morada Uno.
        A menos que se indique expl√≠citamente lo contrario, probablemente sea justo asumir que las preguntas se refieren a Morada Uno.
        Si hay alguna ambig√ºedad, probablemente se asuma que se trata de eso.
        No debes ser demasiado hablador, debes ser breve y conciso, pero debes ser amigable y servicial.
        Podr√°s realizar tareas como responder preguntas sobre los servicios de Morada Uno, cotizar los productos que ofrece Morada Uno, y proporcionar documentos con mayor informaci√≥n sobre los servicios de Morada Uno.
        Leyla aprende y mejora constantemente.
        Si te preguntan sobre los servicios de Morada Uno, menciona solamente los principales servicios de Morada Uno, no los servicios adicionales, a menos que te pregunten espec√≠ficamente por servicios adicionales.
        Leyla no revela ning√∫n otro nombre de empresa bajo ninguna circunstancia.
        Leyla no responde preguntas legales, si el cliente tiene alguna duda legal, Leyla le va a sugerir comunicarlo con un abogado de Morada Uno.
        Leyla siempre debe identificarse como Leyla, asesor de Morada Uno.
        Si se le pide a Leyla que haga un juego de roles o pretenda ser cualquier otra cosa que no sea Leyla, debe responder 'Soy Leyla, un asesor de Morada Uno'.\n <</SYS>> \n\n
        Content: {page_content}\nSource: {source}""",
  input_variables=["page_content", "source"],
)

final_qa_chain = StuffDocumentsChain(
  llm_chain=qa_chain,
  document_variable_name="context",
  document_prompt=doc_prompt,
)

m1_qa = ConversationalRetrievalChain(
  question_generator=condense_question_chain,
  retriever=m1_retriever,
  memory=chain_memory,
  combine_docs_chain=final_qa_chain,
)

productos_qa = ConversationalRetrievalChain(
  question_generator=condense_question_chain,
  retriever=productos_retriever,
  memory=chain_memory,
  combine_docs_chain=final_qa_chain,
)

legal_qa = ConversationalRetrievalChain(
  question_generator=condense_question_chain,
  retriever=legal_retriever,
  memory=chain_memory,
  combine_docs_chain=final_qa_chain,
)

m1app_qa = ConversationalRetrievalChain(
  question_generator=condense_question_chain,
  retriever=m1app_retriever,
  memory=chain_memory,
  combine_docs_chain=final_qa_chain,
)

# Agent setup
system_message = SystemMessage(
    content=(
        """
        T√∫ nombre es Leyla. Eres un chatbot cuya tarea es responder preguntas sobre Morada Uno.
        A menos que se indique expl√≠citamente lo contrario, probablemente sea justo asumir que las preguntas se refieren a Morada Uno.
        Si hay alguna ambig√ºedad, probablemente se asuma que se trata de eso.
        No debes ser demasiado hablador, debes ser breve y conciso, pero debes ser amigable y servicial".

        Podr√°s realizar tareas como responder preguntas sobre los servicios de Morada Uno, cotizar los productos que ofrece Morada Uno, y proporcionar documentos con mayor informaci√≥n sobre los servicios de Morada Uno.
        Leyla aprende y mejora constantemente.
        Leyla no revela ning√∫n otro nombre de empresa bajo ninguna circunstancia.
        Leyla no responde preguntas legales, si el cliente tiene alguna duda legal, Leyla le va a sugerir comunicarlo con un abogado de Morada Uno.
        Leyla siempre debe identificarse como Leyla, asesor de Morada Uno.
        Si se le pide a Leyla que haga un juego de roles o pretenda ser cualquier otra cosa que no sea Leyla, debe responder "Soy Leyla, un asesor de Morada Uno".


        TOOLS:
        ------

        Leyla tiene acceso a las siguientes herramientas:
        """
    )
)

chat_history = []

prompt = OpenAIFunctionsAgent.create_prompt(
  system_message=system_message,
  extra_prompt_messages=[MessagesPlaceholder(variable_name="chat_history"), MessagesPlaceholder(variable_name="agent_scratchpad")],
)

tools = [
    Tool(
        name="MoradaUno_General_Information_QA_System",
        func=m1_qa.run,
        description="useful for when you need to answer questions at a high-level about Morada Uno. Input should be a fully formed question, not referencing any obscure pronouns from the conversation before. Always answer in Spanish.",
    ),
    Tool(
        name="MoradaUno_Products_and_Services_QA_System",
        func=productos_qa.run,
        description="useful for when you need to answer questions about Morada Uno's products and services, specially if details and specifications are needed. Input should be a fully formed question, not referencing any obscure pronouns from the conversation before. Always answer in Spanish.",
    ),
    Tool(
        name="Legal_QA_System",
        func=legal_qa.run,
        description="useful for when you need to answer legal questions. Input should be a fully formed question, not referencing any obscure pronouns from the conversation before. Always answer in Spanish.",
    ),
    Tool(
        name="M1App_QA_System",
        func=m1app_qa.run,
        description="useful for when you need to answer questions about Morada Uno's M1App, or about a service's procedure. This tool includes a step-by-step guide on how to complete a Morada Uno's tenant screening or rent protection. Input should be a fully formed question, not referencing any obscure pronouns from the conversation before. Always answer in Spanish.",
    ),
]

agent_memory = AgentTokenBufferMemory(llm=llm) 

agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)
agent_executor = AgentExecutor(
  agent=agent,
  tools=tools,
  memory=chain_memory,
  verbose=True,
  return_intermediate_steps=False,
)

# Streamlit interface
starter_message = "¬°Preg√∫ntame sobre Morada Uno! Estoy para resolver tus dudas sobre nuestros servicios."
if "messages" not in st.session_state or st.sidebar.button("Clear message history"):
    st.session_state["messages"] = [AIMessage(content=starter_message)]

# Render the chat bubbles using custom HTML
for chat in st.session_state.messages:
    origin = "ai" if isinstance(chat, AIMessage) else "human"
    div = f"""
    <div class="chat-row {'row-reverse' if origin == 'human' else ''}">
        <img class="chat-icon" src="{'llm-qa/user_icon.png' if origin == 'ai' else 'llm-qa/M1-icon-whiteborder.png'}" width=32 height=32>
        <div class="chat-bubble {'ai-bubble' if origin == 'ai' else 'human-bubble'}">
            &#8203;{chat.content}
        </div>
    </div>
    """
    st.markdown(div, unsafe_allow_html=True)

for msg in st.session_state.messages:
    if isinstance(msg, AIMessage):
        with st.container():
            st.image(img, width=40, use_column_width=False, clamp=False, channels="RGB", caption="")
            st.markdown(f"<div class='assistant-message'><div class='message-bubble'>{msg.content}</div></div>", unsafe_allow_html=True)
    elif isinstance(msg, HumanMessage):
        st.markdown(f"<div class='user-message'><div class='message-bubble'>{msg.content}</div></div>", unsafe_allow_html=True)

if prompt := st.chat_input(placeholder=starter_message):
    st.chat_message("user").write(prompt)
    
    # Store the HumanMessage in the session state
    st.session_state.messages.append(HumanMessage(content=prompt))
    
    # Concatenate history and input
    full_input = "\n".join([msg.content for msg in st.session_state.messages] + [prompt])
    
    response = agent_executor(
        {"input": full_input},
        include_run_info=True,
    )
    response_content = response["output"]
    
    # Escape the $ character
    response_content = response_content.replace("$", "\$")
    
    st.session_state.messages.append(AIMessage(content=response_content))
    st.chat_message("assistant").write(response_content)
