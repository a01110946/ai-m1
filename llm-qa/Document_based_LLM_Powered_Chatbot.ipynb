{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq21csMv_bO-"
      },
      "source": [
        "#### **Pip Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hhHgmrmZHDhH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain\n",
        "!pip install chromadb\n",
        "!pip install openai\n",
        "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
        "#!pip install gpt_index==0.4.24\n",
        "!pip install PyPDF2\n",
        "!pip install PyCryptodome\n",
        "!pip install gradio\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu\n",
        "!pip install python-magic\n",
        "!pip install config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3uUvwbF_Xue"
      },
      "source": [
        "### **Cookbook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKROhqxf8StO"
      },
      "source": [
        "##### **Chat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VGohMWxf8GVq"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "chat = ChatOpenAI(temperature=.2, openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFajDi2M8SUi",
        "outputId": "2982653d-4234-4eef-9e47-7c0c491b0f27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='You could try a caprese salad with fresh tomatoes, mozzarella, and basil.', additional_kwargs={})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
        "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkkAeAbp8hRX"
      },
      "source": [
        "##### **Documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5Sa9KCv78d6I"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_dxXCQ38g2W",
        "outputId": "41b4f8f2-f1e7-430a-dd2e-1dab0c64c247"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\", metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
        "         metadata={\n",
        "             'my_document_id' : 234234,\n",
        "             'my_document_source' : \"The LangChain Papers\",\n",
        "             'my_document_create_time' : 1680013019\n",
        "         })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB8VhypE8qt4"
      },
      "source": [
        "##### **Language Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VltHY78X8tZ8"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "M2NU2l-Q8v2q",
        "outputId": "13af3736-e339-4074-dbea-a8e70a1b2f5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nSaturday.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(\"What day comes after Friday?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKSEwOrj9Ays"
      },
      "source": [
        "##### **Text Embedding Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S9jyyeJx9CTf"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J0kzzCHD9GMe"
      },
      "outputs": [],
      "source": [
        "text = \"Hi! It's time for the beach\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki8p35Lg9Pxf"
      },
      "source": [
        "##### **Prompt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2yDnTscS9ToP",
        "outputId": "215327b0-7e42-4878-f5c4-bf53ef368862"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nThe statement is incorrect; tomorrow is Tuesday, not Wednesday.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
        "\n",
        "# I like to use three double quotation marks for my prompts because it's easier to read\n",
        "prompt = \"\"\"\n",
        "Today is Monday, tomorrow is Wednesday.\n",
        "\n",
        "What is wrong with that statement?\n",
        "\"\"\"\n",
        "\n",
        "llm(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UprOJ0su9czl"
      },
      "source": [
        "##### **Prompt Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zDVpKYA9e-N",
        "outputId": "d4963f40-5845-4579-ce38-b91a7c9944cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Prompt: \n",
            "I really want to travel to Rome. What should I do there?\n",
            "\n",
            "Respond in one short sentence\n",
            "\n",
            "-----------\n",
            "LLM Output: Take in the historic sights, explore the ruins, and enjoy some delicious Italian cuisine.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
        "\n",
        "# Notice \"location\" below, that is a placeholder for another value later\n",
        "template = \"\"\"\n",
        "I really want to travel to {location}. What should I do there?\n",
        "\n",
        "Respond in one short sentence\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"location\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(location='Rome')\n",
        "\n",
        "print (f\"Final Prompt: {final_prompt}\")\n",
        "print (\"-----------\")\n",
        "print (f\"LLM Output: {llm(final_prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR7fqQ00977k"
      },
      "source": [
        "##### **Document Loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ifU4JcpL97hl"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import HNLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WCZ7cLIj97XI"
      },
      "outputs": [],
      "source": [
        "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ioF3mK48-C6w"
      },
      "outputs": [],
      "source": [
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKLA15y3-Cyt",
        "outputId": "ef1d86e7-affd-4a68-8a35-f8d577f0d433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 76 comments\n",
            "Here's a sample:\n",
            "\n",
            "Ozzie_osman 84 days ago  \n",
            "             | next [–] \n",
            "\n",
            "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are very pOzzie_osman 84 days ago  \n",
            "             | parent | next [–] \n",
            "\n",
            "Also, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_index) \n"
          ]
        }
      ],
      "source": [
        "print (f\"Found {len(data)} comments\")\n",
        "print (f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14DXmhBj-JJI"
      },
      "source": [
        "##### **Text Splitter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Qvm1QZwA-L-A"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgL8J1fU-L3R",
        "outputId": "f5773346-e921-48bd-f5d1-3ce21adf7a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 1 document\n"
          ]
        }
      ],
      "source": [
        "# This is a long document we can split up.\n",
        "with open('C:/Users/ferna/OneDrive/Documents/Morada Uno_tech/paul_graham_essay.txt') as f:\n",
        "    pg_work = f.read()\n",
        "    \n",
        "print (f\"You have {len([pg_work])} document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qDFwQPM4-L1Q"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 150,\n",
        "    chunk_overlap  = 20,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([pg_work])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG1qsada-LyQ",
        "outputId": "379ec5bd-16a8-4481-8aab-436fcf52d2a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 27317 documents\n",
            "Preview:\n",
            "January 2023 \n",
            "\n",
            "<i>(<a href=\"https://twitter.com/stef/status/1617222428727586816\"><u>Someone</u></a> fed my essays into GPT to make something that could answer\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(texts)} documents\")\n",
        "\"---------------------------------------\"\n",
        "print (\"Preview:\")\n",
        "print (texts[0].page_content, \"\\n\")\n",
        "print (texts[1].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2GNTkaA-cIq"
      },
      "source": [
        "##### **Retrievers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3Sjl9-KA-Lud"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "loader = TextLoader('C:/Users/ferna/OneDrive/Documents/Morada Uno_tech/paul_graham_essay.txt')\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wVU4QZdT-g7Z"
      },
      "outputs": [],
      "source": [
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get embedding engine ready\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "# Embedd your texts\n",
        "db = FAISS.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "J0kLl2YD-gzu"
      },
      "outputs": [],
      "source": [
        "# Init your retriever. Asking for just 1 document back\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2ANyxka-gwf",
        "outputId": "11ab6553-a6bc-44e2-c1a0-95c7198f89bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x000002519EC250C0>, search_type='similarity', search_kwargs={})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gvZ9jGH--goJ"
      },
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJnDWRer-gk_",
        "outputId": "8d327055-9bff-42a6-8507-b2157382d04d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "because users were desperately waiting for what they were building.\n",
            "\n",
            "Instead of telling kids that their treehouses could be on the path\n",
            "to the work they do as adults, we tell them the path goes through\n",
            "school. And unfortunately schoolwork tends to be very different fro\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6-SowCe-vE2"
      },
      "source": [
        "##### **Vector Stores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZG0GMdi2-2BI"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "loader = TextLoader('C:/Users/ferna/OneDrive/Documents/Morada Uno_tech/paul_graham_essay.txt')\n",
        "documents = loader.load()\n",
        "\n",
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get embedding engine ready\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IfJu6VB-1zE",
        "outputId": "d4c99913-0995-4153-9064-06f557a35dd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 4063 documents\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(texts)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Cm3qgP3s-1t_"
      },
      "outputs": [],
      "source": [
        "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haTrC759-1np",
        "outputId": "e75e04fb-938c-40c8-dbeb-5c68e769ad43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 4063 embeddings\n",
            "Here's a sample of one: [0.011870243074141935, -0.013094191805474591, -0.004523136923396113]...\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(embedding_list)} embeddings\")\n",
        "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA9sLtl2_ChQ"
      },
      "source": [
        "##### **Memory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4looApBK-1ed"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "history.add_ai_message(\"hi!\")\n",
        "\n",
        "history.add_user_message(\"what is the capital of france?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erNiu_F9_HSd",
        "outputId": "2b97ffa2-986f-422f-bbd2-e147d1d8840b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content='hi!', additional_kwargs={}),\n",
              " HumanMessage(content='what is the capital of france?', additional_kwargs={})]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgFPoxN-_HKB",
        "outputId": "6e8e96b1-f19b-4434-a506-948c8aea09b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of France is Paris.', additional_kwargs={})"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_response = chat(history.messages)\n",
        "ai_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C11Sn_dH_Mzd",
        "outputId": "029ae8f4-bff7-475e-f7a9-9ad919418129"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content='hi!', additional_kwargs={}),\n",
              " HumanMessage(content='what is the capital of france?', additional_kwargs={}),\n",
              " AIMessage(content='The capital of France is Paris.', additional_kwargs={})]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history.add_ai_message(ai_response.content)\n",
        "history.messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvhcabVL_Os3"
      },
      "source": [
        "##### **Chains**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GutwnJl5_Mvd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgg_gtdY_Qf-"
      },
      "source": [
        "##### **Agents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si0vqILJ_VR4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nDhqOra_xiZ"
      },
      "source": [
        "### **Demo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jSwChRNBQhGN",
        "outputId": "2b494fb8-1330-44c7-a2d6-6879b34b7d44"
      },
      "outputs": [],
      "source": [
        "# # Install package\n",
        "!pip install \"unstructured[local-inference]\"\n",
        "!pip install \"detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\"\n",
        "!pip install layoutparser[layoutmodels,tesseract]\n",
        "!pip install libmagic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6MvvvDh-HrZh"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain.llms import OpenAI\n",
        "#import magic\n",
        "import os\n",
        "import nltk\n",
        "import config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0,\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
        "    model_name=\"gpt-4-32k\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yvyLj7GGHPjK"
      },
      "outputs": [],
      "source": [
        "loader = DirectoryLoader('C:/Users/ferna/OneDrive/Documents/Morada Uno_tech/Demo_docs', glob=\"**/*.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNTZA6bGNNU0",
        "outputId": "54c8acb9-9e49-4dd5-8408-d32cc3b295ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain.document_loaders.directory.DirectoryLoader at 0x27b09de06d0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1ITSD2FmOl4d"
      },
      "outputs": [],
      "source": [
        "loader1 = UnstructuredFileLoader('C:/Users/ferna/OneDrive/Documents/Morada Uno_tech/Demo_docs/eBook de Justicia Alternativa M1.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain.document_loaders.unstructured.UnstructuredFileLoader at 0x27b45e74430>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loader1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WsgB3LZwHZBG"
      },
      "outputs": [],
      "source": [
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzjG3RuaM5uQ",
        "outputId": "57247710-7e57-4f30-ef1a-0e330c40168e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Document(page_content='Descubre la renta perfecta\\n\\n¿Qué es\\n\\nMorada Uno?\\n\\nSomos una empresa mexicana de\\n\\ntecnología, con la misión de\\n\\nempoderar a los profesionales\\n\\ninmobiliarios para lograr\\n\\ntransacciones más rápidas y\\n\\nseguras.\\n\\nSolucionamos los problemas\\n\\nexistentes en el mundo de las\\n\\nrentas, fomentando confianza en\\n\\ncada operación y liquidez para\\n\\ntodo el mercado.\\n\\nNuestro compromiso es brindar a\\n\\nlos arrendadores, arrendatarios y\\n\\nprofesionales inmobiliarios la\\n\\nexperiencia de renta más confiable,\\n\\nflexible y segura.\\n\\nRentas\\n\\nPerfectas\\n\\nToda la protección\\n\\nde una póliza\\n\\njurídica + la certeza\\n\\nde recibir la renta\\n\\npuntual, siempre.\\n\\nTodas las garantías de Morada Uno incluyen:\\n\\nLa mejor investigación del mercado\\n\\nConsulta buró - (inquilino y fiador)\\n\\nConsulta antecedentes legales - (inquilino y fiador)\\n\\nConsulta bases criminales - (inquilino y fiador)\\n\\nInmueble en Registro Público Propiedad (RPP)\\n\\nUn contrato de arrendamiento como debería de ser\\n\\nElaboración contrato Arrendamiento\\n\\nCobertura extinción de dominio\\n\\nAsesoría legal previo a firma\\n\\nFirma digital (Firma electrónica avanzada “WeeSign” / “Docusign”)\\n\\nAcompañamiento en firma física (incl. recabación)\\n\\nRentas garantizadas todo el contrato\\n\\nServicio de cobranza sin costo adicional - puntualidad garantizada\\n\\nGarantía de renta pague o no el inquilino hasta la recuperación del inmueble\\n\\nNos encargamos de recuperar tu inmueble\\n\\nMediación / Negociación\\n\\nProceso extrajudicial\\n\\nProceso judicial\\n\\nConvenio de devolución inmueble\\n\\nAsistencia de un abogado en la entrega del inmueble\\n\\nTodos los gastos legales están cubiertos por Morada Uno - sí todos\\n\\nJuicio de recuperación inmueble por impago rentas\\n\\nJuicio de recuperación inmueble  por abandono\\n\\nJuicio de recuperación inmueble por vencimiento contrato\\n\\nGastos de lanzamiento\\n\\nHonorarios jurídicos\\n\\nGastos legales de procesos jurídicos\\n\\nProtege mejor a\\n\\ntu Cliente con\\n\\nMorada Uno\\n\\nTodos los gastos legales de recuperación del\\n\\ninmueble incluidos.\\n\\nRentas menores a $10,000 serán calculadas\\n\\nsobre $10,000.\\n\\nM3\\n\\nM12\\n\\n3 meses\\n\\nde rentas garantizadas\\n\\n12 meses\\n\\nde rentas garantizadas\\n\\nCosto de\\n\\n30%\\n\\nde un mes de renta\\n\\n+IVA\\n\\nCosto de\\n\\n60%\\n\\nde un mes de renta\\n\\n+IVA\\n\\nM3 Light\\n\\nProtección\\n\\nhasta $10,000/mes\\n\\nDurante 3 meses\\n\\n$5,000 (IVA incluido)\\n\\nTodos los paquetes\\n\\nincluyen\\n\\n+ Investigación (inquilino y fiador) y contratos.\\n\\n+ Protección legal extendida (con extinción de\\n\\ndominio)\\n\\n+ Servicio de cobranza y pago puntual.*\\n\\nCobranza y Pago puntual\\n\\nPago de renta\\n\\nmediante el inquilino\\n\\nPago puntual\\n\\nsiempre\\n\\nEl propietario se encarga de\\n\\ncobrar al inquilino, M1 paga a\\n\\nfin de mes en caso de\\n\\nincumplimiento.\\n\\nM1 paga la renta puntualmente\\n\\ntodos los meses, y se encarga\\n\\nde cobrar al inquilino. ¡Sin\\n\\ncosto adicional!\\n\\nEl propietario cobra\\n\\nla renta\\n\\nLe recordaremos al inquilino que\\n\\nse acerca su fecha límite para\\n\\npagar la renta.\\n\\nCero problemas\\n\\nSin llamadas incómodas, no\\n\\ntendrás que cobrarle al\\n\\ninquilino, lo hacemos por ti.\\n\\nDisfruta la renta perfecta\\n\\nM1 se encarga de todo. Si el inquilino se atrasa\\n\\nnuestro equipo entra en acción automáticamente.\\n\\n01\\n\\n02\\n\\n03\\n\\n04\\n\\n05\\n\\n0606\\n\\n07\\n\\n¡Morada Uno ahora te ofrece un\\n\\nseguro de daños en todas tus rentas!\\n\\nTu propiedad, con todos sus\\n\\nacabados y mobiliario\\n\\nCosto $2,990 / año\\n\\n$1,500 / año en la contratación de cualquier garantía\\n\\nProtección de hasta 5 veces el precio de la renta\\n\\nDescuento por introducción\\n\\nSe puede contratar por separado\\n\\n08\\n\\n¿Cómo funciona Morada Uno?\\n\\nTe acompañamos en todo el proceso para que\\n\\ncierres tu renta lo más pronto posible.\\n\\nPerfil\\n\\n1\\n\\nEl interesado(a) inicia su proceso.\\n\\nLlenar su perfil toma menos de 10 minutos\\n\\nSolo requerimos ID oficial y los\\n\\núltimos 3 comprobantes de ingresos.\\n\\nInvestigación\\n\\nMorada Uno investiga al interesado (a),\\n\\ngenera un reporte en 24 horas o menos\\n\\ny lo envía al asesor.\\n\\n2\\n\\nConfirmación de garantía M1\\n\\n3\\n\\nEl asesor comparte el reporte con el propietario\\n\\npara confirmar la renta; una vez confirmada\\n\\nel propietario carga su información.\\n\\n09\\n\\nContratos\\n\\n4\\n\\nMorada Uno prepara los contratos.\\n\\nInquilino y propietario revisan la versión\\n\\npreeliminar. Una vez confirmados los contratos\\n\\nse envía la versión final.\\n\\nFirma\\n\\nSe agenda la fecha de firma con Morada Uno;\\n\\nlas firmas pueden ser presenciales o electrónicas\\n\\nAl momento de la firma se pagan primer mes de renta,\\n\\ndepósito y la garantía Morada Uno.\\n\\n5\\n\\nNuestras\\n\\nAlianzas\\n\\n10\\n\\n11\\n\\nSeguridad y\\n\\nrespaldo, así\\n\\nde simple\\n\\n0212\\n\\nMorada Uno México SAPI de C. V.\\n\\nTabasco 311 - 403\\n\\nRoma Norte, Cuauhtémoc, 06700\\n\\nCiudad de México\\n\\n55 8526 6367\\n\\nWhatsApp: +52 55 7138 9205\\n\\nwww.moradauno.com.mx\\n\\nmoradaunoMx\\n\\nCon administración de pagos M1 Sin administración de pagos', metadata={'source': 'C:\\\\Users\\\\ferna\\\\OneDrive\\\\Documents\\\\Morada Uno_tech\\\\Demo_docs\\\\Brochure Morada Uno - MX.pdf'})"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(documents))\n",
        "documents[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1QVXLQtYHY9Q"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MWUNBP7iHY4t"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 3950, which is longer than the specified 500\n",
            "Created a chunk of size 593, which is longer than the specified 500\n",
            "Created a chunk of size 705, which is longer than the specified 500\n",
            "Created a chunk of size 586, which is longer than the specified 500\n",
            "Created a chunk of size 706, which is longer than the specified 500\n",
            "Created a chunk of size 635, which is longer than the specified 500\n",
            "Created a chunk of size 783, which is longer than the specified 500\n",
            "Created a chunk of size 615, which is longer than the specified 500\n"
          ]
        }
      ],
      "source": [
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zF45jjUhHYzB"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ],
      "source": [
        "#docsearch = Chroma.from_documents(texts, embeddings)\n",
        "docsearch = Chroma.from_documents(texts, embeddings, metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(texts))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type='stuff', retriever=docsearch.as_retriever(), return_source_documents=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"cuáles son los precios de las garantías de Morada Uno?\"\n",
        "result = qa({\"query\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'El costo mínimo de contratación de cualquier garantía es de $3,600 pesos. Morada Uno ofrece diferentes opciones de garantías de renta, como la garantía de rentas M3 Light, que tiene un precio de $5,000 pesos (IVA incluido) para rentas iguales o menores a $30,000 pesos mensuales, y la garantía de rentas M3, que equivale al 30% de un mes de renta, más IVA. El precio de las garantías puede variar dependiendo del valor mensual de la renta.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'source_documents'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32mfernando-m1\\ai-m1\\llm-qa\\Document_based_LLM_Powered_Chatbot.ipynb Cell 68\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell://github/fernando-m1/ai-m1/llm-qa/Document_based_LLM_Powered_Chatbot.ipynb#Y123sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m result[\u001b[39m\"\u001b[39;49m\u001b[39msource_documents\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
            "\u001b[1;31mKeyError\u001b[0m: 'source_documents'"
          ]
        }
      ],
      "source": [
        "result[\"source_documents\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JYsvRw3HQC7"
      },
      "source": [
        "### **Hide**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "TXo3zUyCSdRg",
        "outputId": "f54e3f31-923a-42ce-a106-d41c39d61fe8"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-3d758eb9f171>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgpt_index\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleDirectoryReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPTListIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPTSimpleVectorIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLLMPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromptHelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_index'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import gradio as gr\n",
        "import sys\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "\n",
        "def construct_index(directory_path):\n",
        "    max_input_size = 4096\n",
        "    num_outputs = 512\n",
        "    max_chunk_overlap = 20\n",
        "    chunk_size_limit = 600\n",
        "\n",
        "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
        "\n",
        "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
        "\n",
        "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
        "\n",
        "    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "\n",
        "    index.save_to_disk('index.json')\n",
        "\n",
        "    return index\n",
        "\n",
        "def chatbot(input_text):\n",
        "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
        "    response = index.query(input_text, response_mode=\"compact\")\n",
        "    return response.response\n",
        "\n",
        "iface = gr.Interface(fn=chatbot,\n",
        "                     inputs=gr.components.Textbox(lines=7, label=\"Enter your text\"),\n",
        "                     outputs=\"text\",\n",
        "                     title=\"Custom-trained AI Chatbot\")\n",
        "\n",
        "index = construct_index(\"docs\")\n",
        "iface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThIDtkFpHQwD"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import OpenAI, VectorDBQA\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import GoogleDriveLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import logging\n",
        "import chromadb.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcURRoYcHfL8"
      },
      "outputs": [],
      "source": [
        "# Load documents from the specified directory using a DirectoryLoader object\n",
        "loader = GoogleDriveLoader(folder_id=\"1t5TU1g33JuCxCAsH8LjjwiDmeZkkimLt\")\n",
        "#loader = DirectoryLoader('/content/Brochure Morada Uno - GDL.pdf', glob='*.pdf')\n",
        "documents = loader.load()\n",
        "\n",
        "# split the text to chuncks of of size 1000\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "# Split the documents into chunks of size 1000 using a CharacterTextSplitter object\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Create a vector store from the chunks using an OpenAIEmbeddings object and a Chroma object\n",
        "embeddings = OpenAIEmbeddings(openai_api_key='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')\n",
        "docsearch = Chroma.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaQjiP5iIQ-4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "1cbadf7e30f156f4e0b68f4bca81fa70237e67691d6ebe573bb449e31d409fb8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
